{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNG806D91lqdh1rA7zDu8o/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arunimad/CCIR_MLENG/blob/main/TA9_(tutorial)_Predicting_pima_indian_diabetes_using_Pytorch_and_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction usign Pytorch"
      ],
      "metadata": {
        "id": "DgmhMbh4T0TO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "PyTorch is an open source machine learning (ML) framework based on the Python programming language and the Torch library.\n",
        "\n",
        "\n",
        "Why Choose PyTorch for ANNs?\n",
        "\n",
        "Simplicity and Flexibility: Its intuitive syntax and dynamic nature make PyTorch a favorite among researchers and developers.\n",
        "\n",
        "Dynamic Computation Graphs: This allows for on-the-fly adjustments to your network, a boon during experimental phases.\n",
        "\n",
        "Community and Support: PyTorch is backed by a robust community, ensuring a wealth of resources and support."
      ],
      "metadata": {
        "id": "dy3JDeH3JY80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step -1: Dataset and Preprocessing**"
      ],
      "metadata": {
        "id": "-GWGLU8IMOnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "2TRWsdXMKhlH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/Arunimad/CCIR_MLENG/main/diabetes.csv')"
      ],
      "metadata": {
        "id": "c63UWq8HKtjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Columns **\n",
        "\n",
        "Number of pregnancies\n",
        "\n",
        "Glucose\n",
        "\n",
        "Blood pressure\n",
        "\n",
        "Skin thickness\n",
        "\n",
        "Insulin\n",
        "\n",
        "BMI\n",
        "\n",
        "Diabetes pedigree function\n",
        "\n",
        "Age\n",
        "\n",
        "Outcome (diabetic or non-diabetic)"
      ],
      "metadata": {
        "id": "kqyXsUb3LH-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "NlAHtpK4QBXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.pairplot(df,hue=\"Outcome\")"
      ],
      "metadata": {
        "id": "e_I_yWHdQGn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "tZIujii_QaCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Outcome', axis=1).values\n",
        "y = df['Outcome'].values"
      ],
      "metadata": {
        "id": "ps8zj5J4K3O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "kjjQ96knK5af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "U26hqmu0K9yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is performed to normalize or standardized all independent variables. Some variables, such as age and salary are totally on different scales, hence may have a different effect on Euclidean distance (there are many other ways to calculate distance such as Manhattan distance). Therefore, all independent variables should be on the same scale.\n",
        "\n",
        "Feature scaling is an essential step as there is going to be a lot of computation in ANN and you wouldnâ€™t want any independent variable to dominate on any other variable.    "
      ],
      "metadata": {
        "id": "41D5XbFZLd8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-2: Defining our ANN architecture**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wOJk5Yc1LzKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DiabetesPredictor class inherits from nn.Module, which is a base class for all neural network modules in PyTorch. We define two hidden layers and an output layer, each with its number of neurons. The forward method specifies how the data flows through the network, using ReLU activation functions for hidden layers and a sigmoid function for the output layer to suit our binary classification task."
      ],
      "metadata": {
        "id": "fzOaq871MIuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DiabetesPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesPredictor, self).__init__()\n",
        "        self.fc1 = nn.Linear(8, 16)  # 8 features, 16 neurons in first hidden layer\n",
        "        self.fc2 = nn.Linear(16, 16) # 16 neurons in second hidden layer\n",
        "        self.output = nn.Linear(16, 1) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.output(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "iv0AnTUILryX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters\n",
        "\n",
        "Is there any specific rule or it should be based on trial and error. There is no specific rule to choose a number of units. It is experimentally based. You have to use different hyperparameters. Here, you can see four, based on several trials. There are many functions that may require based on different approaches. In this blog, you will see just two of the unit and activation functions. For a fully connected hidden layer, the rectifier activation function (relu) is used. This rectifier function is mostly suggested to connect hidden layers. Nonetheless, it is good to have a basic understanding of all activation functions, as things may change based on different datasets.  \n",
        "\n",
        "\n",
        "Adding the output layer in ANN is slightly different than adding a hidden layer. It is important to know the dimension of the output layer. In this dataset, you will be predicting binary variables (0 or 1), hence dimension is one. It means you just need one neuron to predict the final output. Remember, this is an example of a classification approach. Another important change in this layer is the activation function. Here, you can see the use of the sigmoid function, because it not only gives a better prediction than rectifier function but also provide the probabilities. Hence, you will get the prediction that if someone is having diabetes or not, including their probabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "3ERxP_mrL9xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-3 Training the model**\n",
        "\n",
        "Training an ANN in PyTorch involves setting up a loss function, an optimizer, and iterating over the dataset."
      ],
      "metadata": {
        "id": "CqwY3SUVMXYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first instantiate our model and define the loss function (Binary Cross Entropy Loss) and optimizer (Adam). The training loop involves iterating over the data for a specified number of epochs. Each iteration includes a forward pass (calculating the output and loss), a backward pass (computing gradients), and an optimization step (updating the model parameters). We also print the loss at regular intervals to monitor the training progress."
      ],
      "metadata": {
        "id": "MUtUkhE7M7Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiabetesPredictor()\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    # Convert arrays to tensors\n",
        "    inputs = torch.tensor(X_train, dtype=torch.float32)\n",
        "    labels = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels.unsqueeze(1))\n",
        "\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "elV72m1pRmGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary Cross Entropy, also known as Binary Log Loss or Binary Cross-Entropy Loss, is a commonly used loss function in machine learning, particularly in binary classification problems. It is designed to measure the dissimilarity between the predicted probability distribution and the true binary labels of a dataset.\n",
        "\n",
        "The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing.\n",
        "\n",
        "***Arrays and tensors***\n",
        "\n",
        "numpy arrays can be 0,1, 2, and even 3 dimensional\n",
        "\n",
        "When and why do we use tensors instead of numpy arrays?\n",
        "\n",
        "\n",
        "PyTorch tensors are N Dimensional arrays that hold both the value and the gradient. Numpy arrays are N Dimensional arrays that only hold a value. Use a Tensor when you need to keep track of a gradient.\n",
        "\n",
        "The difference between a Numpy array and PyTorch tensor is not that great on a mathematical conceptual level, but different frameworks like to implement their own datatypes for versatility and optimisation.\n"
      ],
      "metadata": {
        "id": "j_91NU3jQwyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step-4: Evaluating the model**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The torch.no_grad() context manager is used to ensure that computations in this block don't track gradients, as we don't need them for evaluation. We calculate the model's accuracy by comparing the predicted class (rounded output of the model) with the actual labels and computing the proportion of correct predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "Cm5fMwiUNEIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    y_predicted = model(torch.tensor(X_test, dtype=torch.float32))\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    acc = y_predicted_cls.eq(torch.tensor(y_test).unsqueeze(1)).sum() / float(y_test.shape[0])\n",
        "    print(f'Accuracy: {acc:.4f}')"
      ],
      "metadata": {
        "id": "mtKMkYYkM2Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thats good for a start!"
      ],
      "metadata": {
        "id": "-FWhr0aZOGbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction using KERAS"
      ],
      "metadata": {
        "id": "7vs4BdvDUGcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us look at Keras!"
      ],
      "metadata": {
        "id": "THHvVeD9OJMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras is a deep learning API written in Python and capable of running on top of either JAX, TensorFlow, or PyTorch."
      ],
      "metadata": {
        "id": "e9Z84A72OL5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ZICytbQ3OqIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random seed\n",
        "numpy.random.seed(7)"
      ],
      "metadata": {
        "id": "SkFKnSeCOtFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loaded pima indians dataset\n",
        "dataset = numpy.loadtxt('https://raw.githubusercontent.com/Arunimad/CCIR_MLENG/main/diabetes.csv', delimiter=\",\", skiprows=1)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "gmXvYWZVOuqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "print(\"input variables (X)\",X)\n",
        "print(\"output class variable (Y)\",Y)"
      ],
      "metadata": {
        "id": "XZLv2DSePILK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "m__cFhvdPMVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "kFcCbbQ2PONi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "b4V2wGivPR79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10)"
      ],
      "metadata": {
        "id": "kDTtU_bxPUVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate predictions\n",
        "predictions = model.predict(X_test)\n",
        "# round predictions\n",
        "rounded = [round(x[0]) for x in predictions]\n",
        "print(rounded)\n"
      ],
      "metadata": {
        "id": "GpDapiNENEcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the model with Testingset\n",
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "metadata": {
        "id": "oEdNRwyCPgra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets touch Backward propogation! (Using Pytorch)\n",
        "\n",
        "\n",
        "The PyTorch backward pass, also known as backward propagation, is a process used to calculate the gradients of a neural network's parameters for the loss function. You can further use this information to update the parameters in the direction that reduces the loss. You can perform the backward pass by calling the .backward() method on the loss tensor. This recursively propagates the error signal back through the network, layer by layer, calculating the gradient of each parameter along the way.\n"
      ],
      "metadata": {
        "id": "ProMQ5qJSPZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/Arunimad/CCIR_MLENG/main/diabetes.csv')\n",
        "\n",
        "X = df.drop('Outcome', axis=1).values\n",
        "y = df['Outcome'].values\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DiabetesPredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesPredictor, self).__init__()\n",
        "        self.fc1 = nn.Linear(8, 16)  # 8 features, 16 neurons in first hidden layer\n",
        "        self.fc2 = nn.Linear(16, 16) # 16 neurons in second hidden layer\n",
        "        self.output = nn.Linear(16, 1) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.output(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "model = DiabetesPredictor()\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    # Convert arrays to tensors\n",
        "    inputs = torch.tensor(X_train, dtype=torch.float32)\n",
        "    labels = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels.unsqueeze(1))\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "OdU2MBYHSqWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    y_predicted = model(torch.tensor(X_test, dtype=torch.float32))\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    acc = y_predicted_cls.eq(torch.tensor(y_test).unsqueeze(1)).sum() / float(y_test.shape[0])\n",
        "    print(f'Accuracy: {acc:.4f}')"
      ],
      "metadata": {
        "id": "pvem46koTFrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened in those extra lines?\n",
        "\n",
        "When you call loss.backward(), all it does is compute gradient of loss w.r.t all the parameters in loss that have requires_grad = True and store them in parameter.grad attribute for every parameter.\n",
        "\n",
        "\n",
        "optimizer.step() updates all the parameters based on parameter.grad\n",
        "\n",
        "loss.backward() # do gradient of all parameters for which we set required_grad= True. parameters could be any variable defined in code\n",
        "\n",
        "optimizer.step() # according to the optimizer function (defined previously in our code), we update those parameters to finally get the minimum loss(error)."
      ],
      "metadata": {
        "id": "X2M9SifrVF-h"
      }
    }
  ]
}